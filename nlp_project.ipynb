{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        words = np.array(list(self.word2vec.keys()))\n",
    "        scores = np.zeros((len(words)))\n",
    "        for i, word in enumerate(words):\n",
    "            scores[i] = self.score(w, word)\n",
    "        idxs = np.argsort(scores)\n",
    "        return words[idxs[-(K+1):-1]]\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        try:\n",
    "            vec1 = self.word2vec[w1]\n",
    "        except KeyError:\n",
    "            vec1 = self.word2vec[w1.capitalize()]\n",
    "        try:\n",
    "            vec2 = self.word2vec[w2]\n",
    "        except KeyError:\n",
    "            vec2 = self.word2vec[w2.capitalize()]\n",
    "        return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7058595452409974\n",
      "germany berlin 0.7060253015336013\n",
      "['dog' 'Cat' 'kitten' 'kitty' 'cats']\n",
      "['pup' 'canine' 'Dog' 'puppy' 'dogs']\n",
      "['canine' 'cats' 'puppies' 'Dogs' 'dog']\n",
      "['Bordeaux' 'Lyon' 'Versailles' 'France' 'Parisian']\n",
      "['German' 'Munich' 'Berlin' 'Bavaria' 'Austria']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=25000)\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            word_list = sent.split(' ')\n",
    "            if idf is False:\n",
    "                weighted_vectors = []\n",
    "                for w in word_list:\n",
    "                    try:\n",
    "                        weighted_vectors.append(self.w2v.word2vec[w])\n",
    "                    except KeyError:\n",
    "                        pass # if the word isn't in the word2vec vocab pass \n",
    "                if weighted_vectors:\n",
    "                    sentemb.append((np.mean(weighted_vectors, axis=0))[None,:])\n",
    "                else:\n",
    "                    sentemb.append(np.zeros((300,))) # if all words in sent aren't in word2vec vocab, encode with an array of zeros\n",
    "            else:\n",
    "                weighted_vectors = []\n",
    "                for w in word_list:\n",
    "                    try:\n",
    "                        weighted_vectors.append(self.w2v.word2vec[w]*idf[w])\n",
    "                    except KeyError:\n",
    "                        pass \n",
    "                if weighted_vectors:\n",
    "                    sentemb.append((np.mean(weighted_vectors, axis=0))[None,:])\n",
    "                else:\n",
    "                    sentemb.append(np.zeros((300,)))\n",
    "        \n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        scores = np.zeros((len(sentences)))\n",
    "        for i, key in enumerate(sentences):\n",
    "            scores[i] = self.score(key, s, idf)\n",
    "        idxs = np.argsort(scores)\n",
    "        output = [(sentences[idx], scores[idx]) for idx in idxs[-(K+1):-1]]\n",
    "        print(output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        emb1 = self.encode([s1], idf)[0]\n",
    "        emb2 = self.encode([s2], idf)[0]\n",
    "        norm1 = np.linalg.norm(emb1)\n",
    "        norm2 = np.linalg.norm(emb2)\n",
    "        if norm1 == 0 or norm2 == 0: # If s1 or s2 was encoded as an array of zeros (contained no words in the word2vec vocab)\n",
    "            score = -1\n",
    "        else:\n",
    "            score = np.dot(emb1, emb2)/(norm1*norm2)\n",
    "        return score\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            sent = sent.split(' ')\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        for word in idf.keys():\n",
    "            idf[word] = max(1, np.log10(len(sentences) / (idf[word])))\n",
    "        return idf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "Input sentence : 1 smiling african american boy .\n",
      "[('a girl in black hat holding an african american baby .', 0.8191122491011851), ('an african american man is sitting .', 0.8207047384633509), ('an afican american woman standing behind two small african american children .', 0.8217139139540895), ('a little african american boy and girl looking up .', 0.8498556003875812), ('an african american man smiling .', 0.9170453357707757)]\n",
      "0.5726258859719607\n",
      "[('a girl in black hat holding an african american baby .', 0.846157865119249), ('an afican american woman standing behind two small african american children .', 0.8546787739055061), ('a little african american boy and girl looking up .', 0.8649160645466021), ('an african american man is sitting .', 0.8710560062973776), ('an african american man smiling .', 0.9215230615776525)]\n",
      "0.4751450875368782\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "with open(os.path.join(PATH_TO_DATA, 'sentences.txt')) as file:\n",
    "    lines = file.readlines()\n",
    "sentences = [line.strip(' \\n') for line in lines]\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('Input sentence : ' + sentences[10])\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf=False)  # BoV-mean\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf=False))\n",
    "\n",
    "\n",
    " \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_file(path, nmax):\n",
    "    output = {}\n",
    "    with io.open(path, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vec = line.split(' ', 1)\n",
    "            output[word] = np.fromstring(vec, sep=' ')\n",
    "            if i == (nmax - 1):\n",
    "                break\n",
    "    return output\n",
    "\n",
    "eng_file = os.path.join(PATH_TO_DATA, 'wiki.en.vec')\n",
    "fr_file = os.path.join(PATH_TO_DATA, 'wiki.fr.vec')\n",
    "nmax = 50000\n",
    "wiki_en = load_file(eng_file, nmax)\n",
    "wiki_fr = load_file(fr_file, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 18970)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "X = []\n",
    "Y = []\n",
    "for word in wiki_fr.keys():\n",
    "    if word in wiki_en.keys():\n",
    "        X.append(wiki_fr[word])\n",
    "        Y.append(wiki_en[word])\n",
    "X = np.array(X).T\n",
    "Y = np.array(Y).T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583.8934480686562\n"
     ]
    }
   ],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "U, s, V = linalg.svd(np.dot(Y, X.T)) \n",
    "W = np.dot(U, V)\n",
    "\n",
    "print(np.linalg.norm(Y-np.dot(W, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to robe : ['dresses', 'gowns', 'robes', 'blouse', 'gown']\n",
      "Most similar to agent : ['fbi', 'informant', 'undercover', 'agents', 'agent']\n",
      "Most similar to jaune : ['bluish', 'pinkish', 'reddish', 'yellowish', 'yellow']\n",
      "Most similar to roi : ['usurper', 'throne', 'kingship', 'vassal', 'king']\n",
      "Most similar to ciel : ['heavens', 'skyscrapers', 'skies', 'sky', 'skyscraper']\n",
      "---------------------------------------------------------------\n",
      "Most similar to story : ['racontant', 'autobiographique', 'raconte', 'récit', 'roman']\n",
      "Most similar to brain : ['cérébrale', 'cérébraux', 'cérébrales', 'cérébral', 'cerveau']\n",
      "Most similar to knee : ['cheville', 'blessure', 'ligaments', 'hanche', 'genou']\n",
      "Most similar to spy : ['espions', 'espionner', 'espionnage', 'espionne', 'espion']\n",
      "Most similar to whale : ['requins', 'phoque', 'phoques', 'baleines', 'baleine']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "french_words = ['robe', 'agent', 'jaune', 'roi', 'ciel']\n",
    "english_words = ['story', 'brain', 'knee', 'spy', 'whale']\n",
    "\n",
    "def most_similar(word, lang='fr', K=5):\n",
    "    if lang == 'fr':\n",
    "        original_embed = wiki_fr[word][:,None]\n",
    "        keys = list(wiki_en.keys())\n",
    "        scores = np.zeros((len(keys)))\n",
    "        embed = np.dot(W, original_embed)\n",
    "        for i, w in enumerate(keys):\n",
    "            emb = wiki_en[w]\n",
    "            scores[i] = np.dot(emb, embed)/(np.linalg.norm(emb)*np.linalg.norm(embed))\n",
    "    elif lang == 'en':\n",
    "        original_embed = wiki_en[word][:,None]\n",
    "        keys = list(wiki_fr.keys())\n",
    "        scores = np.zeros((len(keys)))\n",
    "        embed = np.dot(np.linalg.inv(W), original_embed)\n",
    "        for i, w in enumerate(keys):\n",
    "            emb = wiki_fr[w]\n",
    "            scores[i] = np.dot(emb, embed)/(np.linalg.norm(emb)*np.linalg.norm(embed))\n",
    "    idxs = np.argsort(scores)\n",
    "    output = [keys[idx] for idx in idxs[-K:]]\n",
    "    return output\n",
    "\n",
    "for w in french_words:\n",
    "    print('Most similar to {} : {}'.format(w, most_similar(w, lang='fr')))\n",
    "print('---------------------------------------------------------------')\n",
    "for w in english_words:\n",
    "    print('Most similar to {} : {}'.format(w, most_similar(w, lang='en')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_train_val_file(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with io.open(path) as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            label, sent = line.split(' ', 1)\n",
    "            sentences.append(sent)\n",
    "            labels.append(int(label))\n",
    "    return (sentences, labels)\n",
    "\n",
    "def load_test_file(path):\n",
    "    sentences = []\n",
    "    with io.open(path) as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            sentences.append(line.strip('\\n'))\n",
    "    return sentences\n",
    "\n",
    "train_file = os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train')\n",
    "val_file = os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev')\n",
    "test_file = os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X')\n",
    "train_sent, train_labels = load_train_val_file(train_file)\n",
    "val_sent, val_labels = load_train_val_file(val_file)\n",
    "test_sent = load_test_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "idf_train = s2v.build_idf(train_sent)\n",
    "idf_val = s2v.build_idf(val_sent)\n",
    "train_vec_mean = s2v.encode(train_sent, idf=False)\n",
    "train_vec_idf = s2v.encode(train_sent, idf_train)\n",
    "val_vec_mean = s2v.encode(val_sent, idf=False)\n",
    "val_vec_idf = s2v.encode(val_sent, idf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (average) : 0.5036872293105467\n",
      "Validation accuracy (average) : 0.42363636363636364\n",
      "Training accuracy (idf-weighted) : 0.49549338639822077\n",
      "Validation accuracy (idf-weighted) : 0.4163636363636364\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "model1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=100, max_iter=1000).fit(train_vec_mean, train_labels)\n",
    "print('Training accuracy (average) : {}'.format(model1.score(train_vec_mean, np.array(train_labels))))\n",
    "print('Validation accuracy (average) : {}'.format(model1.score(val_vec_mean, np.array(val_labels))))\n",
    "\n",
    "model2 = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=100, max_iter=1000).fit(train_vec_idf, train_labels)\n",
    "print('Training accuracy (idf-weighted) : {}'.format(model2.score(train_vec_idf, np.array(train_labels))))\n",
    "print('Validation accuracy (idf-weighted) : {}'.format(model2.score(val_vec_idf, np.array(val_labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "test_vec = s2v.encode(test_sent)\n",
    "test_labels = model1.predict(test_vec)\n",
    "output = ['{} \\n'.format(i) for i in test_labels]\n",
    "with open(\"logreg_bov_y_test_sst.txt\", \"w\") as f:\n",
    "    f.writelines(list(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (average) : 0.5311951305162121\n",
      "Validation accuracy (average) : 0.43454545454545457\n",
      "Training accuracy (idf-weighted) : 0.5587030317218775\n",
      "Validation accuracy (idf-weighted) : 0.43454545454545457\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model1 = SVC(kernel='rbf',\n",
    "            decision_function_shape='ovr', C=1000, gamma='auto').fit(train_vec_mean, train_labels)\n",
    "print('Training accuracy (average) : {}'.format(model1.score(train_vec_mean, np.array(train_labels))))\n",
    "print('Validation accuracy (average) : {}'.format(model1.score(val_vec_mean, np.array(val_labels))))\n",
    "\n",
    "model2 = SVC(kernel='rbf',\n",
    "            decision_function_shape='ovr', C=100, gamma='auto').fit(train_vec_idf, train_labels)\n",
    "print('Training accuracy (idf-weighted) : {}'.format(model2.score(train_vec_idf, np.array(train_labels))))\n",
    "print('Validation accuracy (idf-weighted) : {}'.format(model2.score(val_vec_idf, np.array(val_labels))))\n",
    "\n",
    "\n",
    "# Making predictions for test set and saving results\n",
    "test_labels = model2.predict(test_vec)\n",
    "output = ['{} \\n'.format(i) for i in test_labels]\n",
    "with open(\"svm_bov_y_test_sst.txt\", \"w\") as f:\n",
    "    f.writelines(list(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "#PATH_TO_DATA = \"../../data/\"\n",
    "\n",
    "# TYPE CODE HERE\n",
    "train_sent, train_labels = load_train_val_file(train_file)\n",
    "val_sent, val_labels = load_train_val_file(val_file)\n",
    "test_sent = load_test_file(test_file)\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(train_labels)\n",
    "y_val = keras.utils.np_utils.to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=15477, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(train_sent)\n",
    "x_train = tokenizer.texts_to_sequences(train_sent)\n",
    "x_val = tokenizer.texts_to_sequences(val_sent)\n",
    "x_test = tokenizer.texts_to_sequences(test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=50, dtype='int32', padding='pre', value=0.0)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=50, dtype='int32', padding='pre', value=0.0)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=50, dtype='int32', padding='pre', value=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 64  # word embedding dimension\n",
    "nhid       = 128  # number of hidden units in the LSTM\n",
    "vocab_size = 15477  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, input_length=x_train.shape[1]))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 64)            990528    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,089,989\n",
      "Trainable params: 1,089,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/6\n",
      "8543/8543 [==============================] - 68s 8ms/step - loss: 1.5617 - acc: 0.2810 - val_loss: 1.5124 - val_acc: 0.3245\n",
      "Epoch 2/6\n",
      "8543/8543 [==============================] - 59s 7ms/step - loss: 1.3160 - acc: 0.4061 - val_loss: 1.3504 - val_acc: 0.3891\n",
      "Epoch 3/6\n",
      "8543/8543 [==============================] - 59s 7ms/step - loss: 1.0215 - acc: 0.5209 - val_loss: 1.5030 - val_acc: 0.3973\n",
      "Epoch 4/6\n",
      "8543/8543 [==============================] - 63s 7ms/step - loss: 0.7415 - acc: 0.7005 - val_loss: 1.7233 - val_acc: 0.3700\n",
      "Epoch 5/6\n",
      "8543/8543 [==============================] - 64s 7ms/step - loss: 0.4825 - acc: 0.8207 - val_loss: 2.0485 - val_acc: 0.3482\n",
      "Epoch 6/6\n",
      "8543/8543 [==============================] - 62s 7ms/step - loss: 0.3414 - acc: 0.8833 - val_loss: 2.3273 - val_acc: 0.3600\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 32\n",
    "n_epochs = 6\n",
    "history = model.fit(x_train, y_train, batch_size=bs,epochs=n_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Evolution of training and validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "test_labels = np.argmax(model.predict(x_test), axis=1)\n",
    "output = ['{} \\n'.format(i) for i in test_labels]\n",
    "with open(\"logreg_lstm_y_test_sst.txt\", \"w\") as f:\n",
    "    f.writelines(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 52, 64)            96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 162,569\n",
      "Trainable params: 162,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/6\n",
      "8543/8543 [==============================] - 52s 6ms/step - loss: 1.3856 - acc: 0.3781 - val_loss: 1.2948 - val_acc: 0.4045\n",
      "Epoch 2/6\n",
      "8543/8543 [==============================] - 46s 5ms/step - loss: 1.2172 - acc: 0.4644 - val_loss: 1.2686 - val_acc: 0.4500\n",
      "Epoch 3/6\n",
      "8543/8543 [==============================] - 47s 5ms/step - loss: 1.1072 - acc: 0.5176 - val_loss: 1.2814 - val_acc: 0.4518\n",
      "Epoch 4/6\n",
      "8543/8543 [==============================] - 44s 5ms/step - loss: 0.9923 - acc: 0.5704 - val_loss: 1.2928 - val_acc: 0.4536\n",
      "Epoch 5/6\n",
      "8543/8543 [==============================] - 48s 6ms/step - loss: 0.8791 - acc: 0.6381 - val_loss: 1.4494 - val_acc: 0.4409\n",
      "Epoch 6/6\n",
      "8543/8543 [==============================] - 46s 5ms/step - loss: 0.7728 - acc: 0.6884 - val_loss: 1.4947 - val_acc: 0.4445\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.layers import Conv1D, MaxPooling1D, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "\n",
    "max_len = 52\n",
    "\n",
    "def encode(sentences):\n",
    "    '''Transforms list of sentences into numpy array of word2vec embeddings'''\n",
    "    embed = []\n",
    "    for sent in sentences:\n",
    "        sentemb = []\n",
    "        for w in sent:\n",
    "            try:\n",
    "                sentemb.append(w2v.word2vec[w])\n",
    "            except KeyError:\n",
    "                sentemb.append(np.zeros(300,))\n",
    "        sent_len = len(sentemb)\n",
    "        if sent_len < max_len:\n",
    "            for i in range(max_len-sent_len):\n",
    "                sentemb.append(np.zeros(300,))\n",
    "        embed.append(np.vstack(sentemb))\n",
    "    return np.stack(embed)\n",
    "\n",
    "x_train =  [keras.preprocessing.text.text_to_word_sequence(sent) for sent in train_sent]\n",
    "x_val =  [keras.preprocessing.text.text_to_word_sequence(sent) for sent in val_sent]\n",
    "x_test =  [keras.preprocessing.text.text_to_word_sequence(sent) for sent in test_sent]\n",
    "\n",
    "x_train = encode(x_train)\n",
    "x_val = encode(x_val)\n",
    "x_test = encode(x_test)\n",
    "\n",
    "nhid = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 5, activation='relu', padding='same', input_shape=(52,300)))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(nhid, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' \n",
    "optimizer        =  'nadam' \n",
    "metrics_classif  =  ['accuracy']\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())\n",
    "\n",
    "bs = 16\n",
    "n_epochs = 6\n",
    "history = model.fit(x_train, y_train, batch_size=bs,epochs=n_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "test_labels = np.argmax(model.predict(x_test), axis=1)\n",
    "output = ['{} \\n'.format(i) for i in test_labels]\n",
    "with open(\"w2v_1Dconv_LSTM_y_test_sst.txt\", \"w\") as f:\n",
    "    f.writelines(list(output))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
